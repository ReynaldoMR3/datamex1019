{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 1: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer language vs natural (human) language: how they differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe computer language uses small amount of variables and it can't have sintaxis errors to work,\\nthese differs from natural language, because humans can understand different sintaxis and\\nunderstand different sentences\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The computer language uses small amount of variables and it can't have sintaxis errors to work,\n",
    "these differs from natural language, because humans can understand different sintaxis and\n",
    "understand different sentences\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why NLP needs to deconstruct the natural language in order to analyze?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSo i't can understand the rules of the natural language phrase structure\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So i't can understand the rules of the natural language phrase structure\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the biggest problem for computer to understand natural language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe ambiguity of the fundamental types of words, a word can be a verb or a noun deppending\\nof the context, so thats the biggest problem for computers\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The ambiguity of the fundamental types of words, a word can be a verb or a noun deppending\n",
    "of the context, so thats the biggest problem for computers\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a parse tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDifferent phrase structure rules for a language, it reveals how a sentence is constructed\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Different phrase structure rules for a language, it reveals how a sentence is constructed\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the first-generation NLP algorithms (based on semantic rules) different from modern NLP algorithms (based on knowledge graphs and machine learning)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe first generation NLP algorithms were ruled-based and the moderns one are trained with\\ndatabases of human-human chats so they can learn more sophisticated answers\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The first generation NLP algorithms were ruled-based and the moderns one are trained with\n",
    "databases of human-human chats so they can learn more sophisticated answers\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did early-generation chatbots (e.g. Eliza) recognize natual languge? What are the limitations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThey were primary rule-based\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "They were primary rule-based\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do modern chatbots (e.g. IBM Watson, Facebook Messenger Chatbot) recognize natural language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith knowledge graph, parsing and generating text\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "With knowledge graph, parsing and generating text\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is speech recognition? What is its relation to NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI'ts the ability of a computer to recognize the sounds and treat them with parse tree\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I'ts the ability of a computer to recognize the sounds and treat them with parse tree\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does speech recognition work (how do computers translate sounds into texts)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith different algorithms like Fast fourier transform and the formant frequency combinations\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "With different algorithms like Fast fourier transform and the formant frequency combinations\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is speech synthesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIt's like speech recognition but in reverse, it takes a piece of text and breaks it down\\ninto its phonetic components and then play those sounds back to back in a speaker connected\\nto the computer\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "It's like speech recognition but in reverse, it takes a piece of text and breaks it down\n",
    "into its phonetic components and then play those sounds back to back in a speaker connected\n",
    "to the computer\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are essential for NLP algorithms to produce more accurate results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMachine Learning, deep neural networks\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Machine Learning, deep neural networks\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is text mining / text analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nText Mining:\\nIs the process of deriving meaningful information from natural language text\\nit involves the process of structurin input text deriving data evaluating and interpreting\\nthe output \\n\\nText analysis:\\nEssentially is turning the text into data analysis via the application of natural language\\nprocessing \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Text Mining:\n",
    "Is the process of deriving meaningful information from natural language text\n",
    "it involves the process of structurin input text deriving data evaluating and interpreting\n",
    "the output \n",
    "\n",
    "Text analysis:\n",
    "Essentially is turning the text into data analysis via the application of natural language\n",
    "processing \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is sentiment analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSentiment analysis is contextual mining of text \\nwhich identifies and extracts subjective information in source material\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sentiment analysis is contextual mining of text \n",
    "which identifies and extracts subjective information in source material\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the common areas of application of NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSentimental Analysis\\nChatbots\\nSpeech recognition\\nMachine translation\\nSpell checking\\nKeyword searching\\nInformation extraction\\nAdvertisement matching\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sentimental Analysis\n",
    "Chatbots\n",
    "Speech recognition\n",
    "Machine translation\n",
    "Spell checking\n",
    "Keyword searching\n",
    "Information extraction\n",
    "Advertisement matching\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 6 general steps of NLP analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: Tokenization\\nStep 2: Stemming\\nStep 3: Lemmatization\\nStep 4: POS Tags\\nStep 5: Named entry recognition\\nStep 6: Trunking\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 1: Tokenization\n",
    "Step 2: Stemming\n",
    "Step 3: Lemmatization\n",
    "Step 4: POS Tags\n",
    "Step 5: Named entry recognition\n",
    "Step 6: Trunking\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProcess of breaking strings into tokens of small structures\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Process of breaking strings into tokens of small structures\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNormalized words into its based form or root form\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Normalized words into its based form or root form\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is lemmatization? How is lemmatization different from stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIt's almos the same as stemming but it doesn't cut the word just for cutting,\\nit takes into consideration the morphological analysis of the word to do so it needs\\na detailed dictionary wich the algorithm can look through to link the word back\\nto it's original root form with more precision of the grammatical use.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "It's almos the same as stemming but it doesn't cut the word just for cutting,\n",
    "it takes into consideration the morphological analysis of the word to do so it needs\n",
    "a detailed dictionary wich the algorithm can look through to link the word back\n",
    "to it's original root form with more precision of the grammatical use.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are POS tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPath of speech, it indicates how a word functions in meaning as well as grammatically\\nwithin the sentence a word can have more than one part of speech based on the context in wich\\nit's used\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Path of speech, it indicates how a word functions in meaning as well as grammatically\n",
    "within the sentence a word can have more than one part of speech based on the context in wich\n",
    "it's used\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does named entity recognition do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIt's the process of detecting the named entities, it has three steps.\\n1.- Noun phrase identification\\n2.- The phrase classification\\n3.- Entity disambiguation\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "It's the process of detecting the named entities, it has three steps.\n",
    "1.- Noun phrase identification\n",
    "2.- The phrase classification\n",
    "3.- Entity disambiguation\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does trunking do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPicking up individual pieces of information and grouping them into bigger pieces\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Picking up individual pieces of information and grouping them into bigger pieces\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Ironhack is a Global Tech School ranked num 2 worldwide. Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do. This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course. We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack is a Global Tech School ranked num 2 worldwide.',\n",
       " 'Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.',\n",
       " 'This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course.',\n",
       " 'We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Global',\n",
       " 'Tech',\n",
       " 'School',\n",
       " 'ranked',\n",
       " 'num',\n",
       " '2',\n",
       " 'worldwide',\n",
       " '.',\n",
       " 'Our',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'help',\n",
       " 'people',\n",
       " 'transform',\n",
       " 'their',\n",
       " 'careers',\n",
       " 'and',\n",
       " 'join',\n",
       " 'a',\n",
       " 'thriving',\n",
       " 'community',\n",
       " 'of',\n",
       " 'tech',\n",
       " 'professionals',\n",
       " 'that',\n",
       " 'love',\n",
       " 'what',\n",
       " 'they',\n",
       " 'do',\n",
       " '.',\n",
       " 'This',\n",
       " 'ideology',\n",
       " 'is',\n",
       " 'reflected',\n",
       " 'in',\n",
       " 'our',\n",
       " 'teaching',\n",
       " 'practices',\n",
       " ',',\n",
       " 'which',\n",
       " 'consist',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nine-weeks',\n",
       " 'immersive',\n",
       " 'programming',\n",
       " ',',\n",
       " 'UX/UI',\n",
       " 'design',\n",
       " 'or',\n",
       " 'Data',\n",
       " 'Analytics',\n",
       " 'course',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'a',\n",
       " 'one-week',\n",
       " 'hiring',\n",
       " 'fair',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'helping',\n",
       " 'our',\n",
       " 'students',\n",
       " 'change',\n",
       " 'their',\n",
       " 'career',\n",
       " 'and',\n",
       " 'get',\n",
       " 'a',\n",
       " 'job',\n",
       " 'straight',\n",
       " 'after',\n",
       " 'the',\n",
       " 'course',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'present',\n",
       " 'in',\n",
       " '8',\n",
       " 'countries',\n",
       " 'and',\n",
       " 'have',\n",
       " 'campuses',\n",
       " 'in',\n",
       " '9',\n",
       " 'locations',\n",
       " '-',\n",
       " 'Madrid',\n",
       " ',',\n",
       " 'Barcelona',\n",
       " ',',\n",
       " 'Miami',\n",
       " ',',\n",
       " 'Paris',\n",
       " ',',\n",
       " 'Mexico',\n",
       " 'City',\n",
       " ',',\n",
       " 'Berlin',\n",
       " ',',\n",
       " 'Amsterdam',\n",
       " ',',\n",
       " 'Sao',\n",
       " 'Paulo',\n",
       " 'and',\n",
       " 'Lisbon',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: Preparing Text Data For Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(s):\n",
    "    \"\"\"\n",
    "    Cleans up numbers, URLs, and special characters from a string.\n",
    "\n",
    "    Args:\n",
    "        s: The string to be cleaned up.\n",
    "\n",
    "    Returns:\n",
    "        A string that has been cleaned up.\n",
    "    \"\"\"\n",
    "    s = re.sub('https?:\\/\\/.{1,}[a-z]',' ',s )\n",
    "    s = re.sub('[^a-zA-Z]', ' ', s)\n",
    "    \n",
    "    return s.rstrip().lstrip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_test = \"@Ironhack's-#Q website 776-is http://ironhack.com [(2018)]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ironhack s  q website     is'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = clean_up(try_test)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    \"\"\"\n",
    "    Tokenize a string.\n",
    "\n",
    "    Args:\n",
    "        s: String to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        A list of words as the result of tokenization.\n",
    "    \"\"\"\n",
    "    return word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 's', 'q', 'website', 'is']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenize(new)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_and_lemmatize(l):\n",
    "    \"\"\"\n",
    "    Perform stemming and lemmatization on a list of words.\n",
    "\n",
    "    Args:\n",
    "        l: A list of strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings after being stemmed and lemmatized.\n",
    "    \"\"\"\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    porter = PorterStemmer()\n",
    "    stem = [porter.stem(i) for i in l]\n",
    "    new = [lemmatizer.lemmatize(e) for e in stem]\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 's', 'q', 'websit', 'is']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aftlem = stem_and_lemmatize(token)\n",
    "aftlem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(l):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a list of strings.\n",
    "\n",
    "    Args:\n",
    "        l: A list of strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings after stop words are removed.\n",
    "    \"\"\"\n",
    "    stop = set(stopwords.words('english'))\n",
    "    new = [w for w in l if not w in stop]\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 'q', 'websit']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_stop = remove_stopwords(aftlem)\n",
    "wo_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are features generated in NB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe features are generated based on the conditions that several factors that doesn't depend on \\neach other are seeing together.\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The features are generated based on the conditions that several factors that doesn't depend on \n",
    "each other are seeing together.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does NB expect the features are independent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBecause if they don't depend on each other the probability of seeing them together\\nis just the product of the probabilities \\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Because if they don't depend on each other the probability of seeing them together\n",
    "is just the product of the probabilities \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is posterial probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe probability an event will hapen after all evidence or background information has been\\ntaken into account\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The probability an event will hapen after all evidence or background information has been\n",
    "taken into account\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it called naive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBasically because it makes assuptioms that may or may not turn out to be correct\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Basically because it makes assuptioms that may or may not turn out to be correct\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the pros and cons of NB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPros:\\n    - It's easy and fast to predict a class of test data set\\n    - Naives Bayes classifier performs better compare to other models assuming independence\\n    - It performs well in case of a categorical input variables compared to numerical variables\\n\\nCons:\\n    - If categorical data has a category in a test dataset wich is not observed in the training\\n    data set then the model will asign a 0 probability (zero frequency)\\n    - Naive bayes is known as a bad estimator\\n    - Assumption of independen predictors\\n    \\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Pros:\n",
    "    - It's easy and fast to predict a class of test data set\n",
    "    - Naives Bayes classifier performs better compare to other models assuming independence\n",
    "    - It performs well in case of a categorical input variables compared to numerical variables\n",
    "\n",
    "Cons:\n",
    "    - If categorical data has a category in a test dataset wich is not observed in the training\n",
    "    data set then the model will asign a 0 probability (zero frequency)\n",
    "    - Naive bayes is known as a bad estimator\n",
    "    - Assumption of independen predictors\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the areas of application of NB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCredit scoring \\nMedical data classification\\nReal-time prediction\\nMulti class prediction features\\nText classifications, spam filtering and sentimental analysis\\nRecommendation systems\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Credit scoring \n",
    "Medical data classification\n",
    "Real-time prediction\n",
    "Multi class prediction features\n",
    "Text classifications, spam filtering and sentimental analysis\n",
    "Recommendation systems\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "sf = zipfile.ZipFile('Sentiment140.csv.zip')\n",
    "df = pd.read_csv(sf.open('Sentiment140.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      "target    1600000 non-null int64\n",
      "id        1600000 non-null int64\n",
      "date      1600000 non-null object\n",
      "flag      1600000 non-null object\n",
      "user      1600000 non-null object\n",
      "text      1600000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df['text'].apply(clean_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df['text_processed'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df['text_processed'].apply(stem_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df['text_processed'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 7 columns):\n",
      "target            1600000 non-null int64\n",
      "id                1600000 non-null int64\n",
      "date              1600000 non-null object\n",
      "flag              1600000 non-null object\n",
      "user              1600000 non-null object\n",
      "text              1600000 non-null object\n",
      "text_processed    1600000 non-null object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 85.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagofwords=[]\n",
    "for i in df['text_processed']:\n",
    "    for e in i:\n",
    "        bagofwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist, FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "cfdist = FreqDist(word for word in bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'go': 137942, 'get': 110083, 'day': 108637, 'wa': 103980, 'good': 91700, 'thi': 91664, 'work': 87362, 'like': 82496, 'love': 81018, 'quot': 72338, ...})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5000 = dict(cfdist.most_common()[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(top5000.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    s = SentimentIntensityAnalyzer().polarity_scores(\" \".join(document))\n",
    "    if s['pos'] > 0.3:\n",
    "        s = True\n",
    "    else:\n",
    "        s = False\n",
    "        \n",
    "    return (features, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = df.text_processed[:2000].apply(find_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes accurracy: 75.7\n",
      "Most Informative Features\n",
      "                    love = True             True : False  =     18.2 : 1.0\n",
      "                    wish = True             True : False  =     13.2 : 1.0\n",
      "                     lol = True             True : False  =     10.8 : 1.0\n",
      "                     wow = True             True : False  =     10.3 : 1.0\n",
      "                     win = True             True : False  =     10.3 : 1.0\n",
      "                 support = True             True : False  =     10.3 : 1.0\n",
      "                    sigh = True             True : False  =     10.3 : 1.0\n",
      "                    move = True             True : False  =     10.3 : 1.0\n",
      "                      ok = True             True : False  =      9.7 : 1.0\n",
      "                    cool = True             True : False  =      9.7 : 1.0\n",
      "                    hope = True             True : False  =      9.6 : 1.0\n",
      "                     fun = True             True : False  =      8.2 : 1.0\n",
      "                    good = True             True : False  =      7.7 : 1.0\n",
      "                   angel = True             True : False  =      7.4 : 1.0\n",
      "                    ruin = True             True : False  =      7.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "training_set = feature[:1000]\n",
    "testing_set = feature[1000:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print('Naive bayes accurracy:', (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
